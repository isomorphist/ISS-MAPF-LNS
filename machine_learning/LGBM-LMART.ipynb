{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b0e3a0-2768-4525-be92-d2796ebf2906",
   "metadata": {},
   "source": [
    "# LightGBM LamdaMART: Tuning, Training, Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bccb71-6e72-4913-b1a3-721766f07e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import ndcg_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "# Folder path with pattern\n",
    "folder_pattern = 'train_data/random*-k-050-ns-08-initsol-030-runs-041'\n",
    "\n",
    "# Size of train/test split\n",
    "train_size = 0.9\n",
    "\n",
    "# Read and concatenate data\n",
    "all_data = []  # Initialize as an empty list\n",
    "group_data = []\n",
    "\n",
    "# Using glob to match the file pattern\n",
    "for file_path in sorted(glob.glob(os.path.join(folder_pattern, '*.scen'))):\n",
    "    file_data = pd.read_csv(file_path, sep=' ', header=None)\n",
    "    \n",
    "    # Sorting the DataFrame based on the first column (labels) in descending order\n",
    "    file_data = file_data.sort_values(by=0, ascending=False)\n",
    "\n",
    "    group_data.append(len(file_data))\n",
    "    all_data.append(file_data)  # Append DataFrame to the list\n",
    "\n",
    "all_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "train_data = pd.DataFrame(all_data, columns=None)\n",
    "group_data = pd.Series(group_data)\n",
    "\n",
    "# Data validation checks\n",
    "print('Check if train_data is correct:\\n')\n",
    "print(\"train_data contains NaN values:\\t\\t\", train_data.isna().any().any())\n",
    "print(\"train_data contains inf or -inf values: \", train_data.isin([np.inf, -np.inf]).any().any())\n",
    "contains_strings = train_data.apply(lambda col: col.apply(lambda x: isinstance(x, str)))\n",
    "print(\"train_data contains strings:\\t\\t\", contains_strings.any().any())\n",
    "\n",
    "# Expand the group_data to match train_data's rows\n",
    "expanded_group_data = np.repeat(np.arange(len(group_data)), group_data)\n",
    "\n",
    "# Split functions\n",
    "def group_train_test_split(X, y, groups, train_size, random_seed=42):\n",
    "    assert len(groups) == len(X) == len(y)\n",
    "    unique_groups = np.unique(groups)\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(unique_groups)\n",
    "\n",
    "    train_group_num = int(len(unique_groups) * train_size)\n",
    "    train_groups = unique_groups[:train_group_num]\n",
    "    test_groups = unique_groups[train_group_num:]\n",
    "\n",
    "    train_indices, test_indices = [], []\n",
    "    for idx, group in enumerate(groups):\n",
    "        if group in train_groups:\n",
    "            train_indices.append(idx)\n",
    "        else:\n",
    "            test_indices.append(idx)\n",
    "\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "\n",
    "def get_group_counts(indices, expanded_group_data):\n",
    "    groups_for_indices = expanded_group_data[indices]\n",
    "    _, counts = np.unique(groups_for_indices, return_counts=True)\n",
    "    return counts\n",
    "\n",
    "# Data splitting\n",
    "X_train, X_test, y_train, y_test = group_train_test_split(train_data.iloc[:, 1:], train_data.iloc[:, 0], expanded_group_data, train_size)\n",
    "\n",
    "print(\"\\nTrain/Test split:\", int(round(train_size*100, 2)), \"/\", int(round((1-train_size)*100, 2)))\n",
    "print(\"\\nTrain data size:\\t\\t\", X_train.shape)\n",
    "print(\"Test data size:\\t\\t\\t\", X_test.shape)\n",
    "print(\"Train group size:\\t\\t\", get_group_counts(X_train.index, expanded_group_data).size)\n",
    "print(\"Test group size:\\t\\t\", get_group_counts(X_test.index, expanded_group_data).size)\n",
    "print(\"Number items in each group:\\t\", get_group_counts(X_test.index, expanded_group_data)[0])\n",
    "\n",
    "\n",
    "\n",
    "def custom_eval(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    # Assuming binary relevance; adjust based on your relevance definition\n",
    "    binary_relevance = (labels > 0).astype(int)\n",
    "    auc_score = roc_auc_score(binary_relevance, preds)\n",
    "    # Return a tuple (name of the metric, value, whether higher is better)\n",
    "    return 'custom_auc', auc_score, True\n",
    "\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    boosting_type = trial.suggest_categorical('boosting_type', ['gbrt'])#, 'rf', 'dart'])\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'eval_at': [1, 2, 3],\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': boosting_type,\n",
    "        'n_jobs': 4,\n",
    "        #'label_gain': [1]*27 + [2,3,4], # consider testing other configurations to see if a different emphasis on score 29 changes the model's behavior.\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 60, 100),  # 31, A higher number of leaves can lead to overfitting\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.008, 0.012), # 0.1,  Smaller learning rates lead to better generalization\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 0.75), # 1\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.8, 0.85), # 1\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 2, 3), # 0\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.4, 0.9), # 0\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.9, 1.7), # 0\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 30, 40), # 20, Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting\n",
    "        'max_bin': trial.suggest_int('max_bin', 230, 255), # 255, Smaller values can lead to underfitting while larger values can cause overfitting\n",
    "        'bin_construct_sample_cnt': trial.suggest_int('bin_construct_sample_cnt', 180000, 240000), # 200000\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 12), # -1, deeper and shallower trees can have different impacts on model performance\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 0.9), # 0.001\n",
    "        'feature_fraction_bynode': trial.suggest_float('feature_fraction_bynode', 0.6, 0.9), # 1.0\n",
    "        'extra_trees': trial.suggest_categorical('extra_trees', [True, False]), # false\n",
    "        'path_smooth': trial.suggest_float('path_smooth', 0.5, 1.0), # 0.0       \n",
    "    }  \n",
    "    \n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 600, 1200) # 100, Increasing the number of boosting rounds (iterations) can sometimes \n",
    "    # help, especially if the learning rate is low. Be cautious of overfitting, and use early stopping criteria to prevent it.\n",
    "\n",
    "    if boosting_type != 'dart':\n",
    "        early_stopping_callback = lgb.early_stopping(stopping_rounds=100, verbose=False)\n",
    "    else:\n",
    "        early_stopping_callback = []\n",
    "    \n",
    "    scores = []\n",
    "    ndcg_scores = []\n",
    "    auc_scores = []\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    for train_idx, valid_idx in gkf.split(X_train, y_train, groups=expanded_group_data[X_train.index]):\n",
    "        X_train_fold = X_train.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        qids_train_fold = get_group_counts(train_idx, expanded_group_data)\n",
    "        X_valid_fold = X_train.iloc[valid_idx]\n",
    "        y_valid_fold = y_train.iloc[valid_idx]\n",
    "        qids_valid_fold = get_group_counts(valid_idx, expanded_group_data)\n",
    "\n",
    "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold, group=qids_train_fold)#, weight=weights_train_fold)\n",
    "        valid_data = lgb.Dataset(X_valid_fold, label=y_valid_fold, group=qids_valid_fold, reference=train_data)\n",
    "\n",
    "        model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=num_boost_round, \n",
    "                          callbacks=[early_stopping_callback] if early_stopping_callback else [])\n",
    "\n",
    "        preds = model.predict(X_valid_fold)\n",
    "        score = ndcg_score(y_valid_fold.values.reshape(1, -1), preds.reshape(1, -1), k=3)\n",
    "        scores.append(score)\n",
    "\n",
    "    return -np.mean(scores)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=25000, n_jobs=5)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'eval_at': [1, 2, 3],\n",
    "    #'label_gain': [1]*27 + [2,3,4],\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': best_params['boosting_type'],\n",
    "    'n_jobs': 4,\n",
    "})\n",
    "\n",
    "num_boost_round = best_params.pop('num_boost_round', None)\n",
    "\n",
    "# Print best_params\n",
    "print(\"Best parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=get_group_counts(X_train.index, expanded_group_data))\n",
    "\n",
    "final_model = lgb.train(best_params, train_data, num_boost_round=num_boost_round)\n",
    "\n",
    "test_preds = final_model.predict(X_test)\n",
    "test_score = ndcg_score(y_test.values.reshape(1, -1), test_preds.reshape(1, -1), k=1)\n",
    "\n",
    "print(\"\\nNDCG@1 on the test set:\", test_score)\n",
    "\n",
    "test_score = ndcg_score(y_test.values.reshape(1, -1), test_preds.reshape(1, -1), k=3)\n",
    "print(\"\\nNDCG@3 on the test set:\", test_score)\n",
    "\n",
    "final_model.save_model('trained_ml_model.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
